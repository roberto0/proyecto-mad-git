{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Actividad 1: Agrupar datos con algoritmos de Clustering en Python</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #1c75c8; background-color: #c5ddf6;\">\n",
    "<h2> Preámbulo</h2>\n",
    "<p> Esta activad se inspira de ejercicios disponibles en los recursos siguientes:<p>\n",
    "<ul>\n",
    "<li> Guías y código Python de <a href=\"http://brandonrose.org/\">Brandon Rose</li>\n",
    "<li>Curso de <a href=\"https://www.datacamp.com/courses/unsupervised-learning-in-python\">DataCamp</a> y código disponible en la cuenta GitHub de <a href=\"https://github.com/benjaminwilson/python-clustering-exercises\">Benjamin Wilson</a></li>\n",
    "</ul>\n",
    "<p> La actividad requiere el uso de Python 3.x y <a href=\"http://jupyter.org/install\">Jupyter Notebook</a>. El código entregado fue probado con Python 3.6.1. Para saber cuál versión de Python usted está utilizando, ejecutar la celda siguiente (está información es importante cuando se necesitará instalar nuevos paquetes.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 :: Anaconda 4.2.0 (x86_64)\r\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #D24747; background-color:#F8B4B4\">\n",
    "<h2>Objetivos de la actividad</h2>\n",
    "<p>El <b>objetivo general</b> de esta actividad consiste en saber explorar la estructura oculta de un dataset, implementando una metodología de agrupamiento de datos clásico, utilizando unos algoritmos de clustering estándares (K-means, Ward clustering, DBSCAN) sobre datos estructurados y no estructurados y describiendo sus principales características.</p>  \n",
    "\n",
    "<p> Un <i>objetivo secundario</i> consiste en programar con algunas librerías Python para analizar y visualizar datos (<a href=\"https://pandas.pydata.org/\">Pandas</a>, Sci-Kit learn, Matplotlib, etc.)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>0. Antes de empezar: unas palabras sobre las herramientas de Python para la Ciencia de los Datos...</h2>\n",
    "\n",
    "<img src=\"python-packages.png\"></img>\n",
    "\n",
    "<p>Cada toolkit de Python tiene sus propios objetivos:</p>\n",
    "<ul>\n",
    "     <li><b>Numpy</b> agrega funcionalidades en Python para soportar arreglos y matrices de gran tamaño y funciones matemáticas para manipularlas.</li>\n",
    "     <li><b>SciPy</b> es una colección de algoritmos matemáticos y funciones programadas con NumPy. Agrega funciones y clases de alto nivel para facilitar la manipulación y visualización de datos.</li>\n",
    "      <li><b>Pandas</b> ofrece estructuras de datos y operaciones para manipular y analizar matrices de datos numéricos y series de tiempo.</li>\n",
    "    <li><b>Scikit-learn</b> es una librería Python para el Machine Learning, contiene una implementación de los principales algoritmos estandares para el aprendizaje supervisado y no supervisado.</li>\n",
    "</ul>\n",
    "\n",
    "<p> En la versión actual de Scikit-lean, se puede encontrar en particular los algoritmos de clustering siguiente:</p>\n",
    "<img src=\"clustering-algorithm.png\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Ejercicio 1: descubrir K-means sobre datos estructurados bi-dimensionales</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>El primer dataset que queremos explorar consiste en un archivo CSV donde se encuentra un conjunto 300 observaciones (o instancias) descritas por 2 características numéricas. \n",
    "<br>Ejemplo:<i>1.70993371252,0.698852527208</i></p>\n",
    "<ul><li>La primera etapa consiste en cargar los datos en un objeto <i>DataFrame</i>. Un DataFrame es una de las estructuras de datos provistas por Pandas para representar los datos, consiste en una matriz en dos dimensiones (ver <a href=\"https://pandas.pydata.org/pandas-docs/stable/dsintro.html\">más detalles</a>) donde cada fila es un dato y cada columna una característica sobre los datos.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv('...')#TODO\n",
    "#mostrar la dataframe\n",
    "#dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li>Para tener una primera comprensión de nuestros datos, queremos visualizarlos en un <i><a href=\"https://en.wikipedia.org/wiki/Scatter_plot\">Scatter plot</a></i>, a través la librería Matplotlib:</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Crear un arreglo 'coordinates_x' que contiene los valores de la columna 0 de nuestro dataframe\n",
    "coordinates_x = dataframe.values[:,0]\n",
    "#lo mismo con los valores de la columna 1 del dataframe\n",
    "coordinates_y = dataframe.values[:,1]\n",
    "\n",
    "#Crear y mostrar el scatter plot pasando las coordinadas como parametros de la función plt.scatter().\n",
    "plt.scatter(coordinates_x, coordinates_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Como pueden verlo, nuestro dataset tiene una estructura bastante simple y explicita, aparecen 3 grupos de datos (o <i>clústers</i>). Sin embargo, este caso es particularmente simple ya que los datos tienen solamente 2 dimensiones y que los clústers están bien separados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo K-means (o algoritmo de Lloyd) es un método de agrupamiento, que tiene como objetivo la partición de un conjunto de n observaciones en k grupos en el que cada observación pertenece al grupo cuyo valor medio es más cercano. El problema es computacionalmente difícil (NP-hard). Sin embargo, hay eficientes heurísticas que se emplean comúnmente y convergen rápidamente a un óptimo local (ver <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">más detalles</a>).\n",
    "<ul><li>La librería SciKit-learn de Python ofrece una implementación de este algoritmo, que se puede utilizar con la API siguiente:</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#Declaración de un modelo de clustering especificando el número a priori de clusters que queremos encontrar. \n",
    "##En este caso, hemos elegido por casualidad n_clusters=5.\n",
    "modelKmeans = KMeans(...)#TODO\n",
    "#Entrenamiento del modelo de clustering con los datos de nuestro dataframe\n",
    "modelKmeans.fit(dataframe.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #000000;\">\n",
    "<b>Preguntas:</b>\n",
    "<ol>\n",
    "<li> ¿Cuáles son las etapas del algoritmo de Lloyd?</li>\n",
    "<li> ¿Por qué es necesario initializar varias veces el algoritmo? De qué sirve el parametro n_init?</li>\n",
    "<li> ¿Cómo elegir el número de inicializaciones e iteraciones? (n_init y max_iter)</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<ul>\n",
    "<li>Ahora queremos visualizar cómo el algoritmo agrupó los datos en 5 grupos:\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Crear un arreglo de datos donde cada valor corresponde a la decision del modelo K-Means a la pregunta siguiente:\n",
    "##¿A qué clúster pertenece el dato corriente de la dataframe?\n",
    "labels = modelKmeans.predict(dataframe.values)\n",
    "#print(labels)\n",
    "\n",
    "#Crear un Scatter Plot donde cada punto tiene un color asociado a un grupo\n",
    "plt.scatter(dataframe.values[:,0], dataframe.values[:,1], c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li>Se puede utilizar el mismo modelo para clasificar nuevos datos. NB: Sin embargo, si el objetivo aplicativo consiste en clasificar datos según ciertas categorías es recomendable seguir una metodología de aprendizaje supervisado.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cargar un dataset con nuevos datos\n",
    "dataframe2 = pd.read_csv('...') #TODO\n",
    "\n",
    "#Utilizar el modelo K-Means anterior para clasificar los nuevos datos\n",
    "labels2 = modelKmeans.predict(dataframe2.values)\n",
    "\n",
    "#Visualizar el resultado de la predicción en un Scatter Plot\n",
    "plt.scatter(dataframe2.values[:,0], dataframe2.values[:,1], c=labels2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #000000;\">\n",
    "<b>Preguntas:</b>\n",
    "<ol>\n",
    "<li> ¿Cómo el algoritmo de Lloyd/K-means permitió predecir la clase de los nuevos datos?</li>\n",
    "<li> ¿Cómo se podría definir el concepto de <i>'centroid'</i>?</li>\n",
    "<li> ¿Cuáles son los limites del método que utiliza K-means para calcular los <i>'centroid'</i>?</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<ul><li>Visualizemos los <i>centroids</i> de cada clúster:</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#en el API del modelo k-means existe un metodo permitiendo de obtener un arreglo de datos correspondiendo a los centroids \n",
    "centroids = modelKmeans.cluster_centers_\n",
    "\n",
    "#Dibujamos el Scatter Plot de la dataframe inicial ...\n",
    "plt.scatter(...) #TODO\n",
    "#...y agregamos los centroids en el mismo plot\n",
    "plt.scatter(centroids[:,0], centroids[:,1], marker='o', s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li>La distancia con el centroid permite clasificar los nuevos datos:</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nuevos datos de la dataframe2\n",
    "plt.scatter(dataframe2.values[:,0], dataframe2.values[:,1], c=labels2)\n",
    "#mismos centroids\n",
    "plt.scatter(centroids[:,0], centroids[:,1], marker='...', s=200)#TODO\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #000000;\">\n",
    "<b>Preguntas:</b>\n",
    "<ol>\n",
    "<li> ¿Existe un número de clúster mejor que los otros para buscar la estructura oculta de los datos?</li>\n",
    "<li> ¿Cómo determinar cuál es el mejor número de clúster?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Existen varios métodos estadísticos para determinar el mejor número de clústers tales como los métodos <i>Elbow</i>, <i>Average Silhouette</i> y <i>Gap Statistics</i> (ver <a href=\"http://www.sthda.com/english/wiki/print.php?id=239#three-popular-methods-for-determining-the-optimal-number-of-clusters\">detalles</a>). En la API de la librería SciKit-Learn también existe un método llamado <i>inertia</i> que permite estimar el mejor número k:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_k = range(1, ...) #TODO\n",
    "inertias = []\n",
    "\n",
    "for k in num_k:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "\n",
    "    # Fit model to samples\n",
    "    model.fit(dataframe)\n",
    "\n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot ks vs inertias\n",
    "plt.plot(num_k, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(num_k)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"border: 2px solid #000000;\">\n",
    "<b>Preguntas:</b>\n",
    "<ol>\n",
    "<li> ¿A qué método para buscar el mejor número de clústers corresponde el método <i>inertia</i> de Sci-Kit?</li>\n",
    "<li> ¿Cuáles son las principales <b>ventajas</b> del algoritmo K-means?</li>\n",
    "<li> ¿Cuáles son las principales <b>limites</b> del algoritmo K-means?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Ejercicio 2: Descubrir los algoritmos de clustering jerárquico sobre datos estructurados multi-dimensionales</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<div style=\"float:left;width:45%;\" >\n",
    "    <p>En este segundo ejercicio, queremos explorar otra familia de algoritmos de clustering basada sobre la idea que en ciertos casos los datos pueden tener <b>relaciones jerarquícas</b> ocultas. El Algoritmo de Ward es parte de este grupo de algoritmos.</p>\n",
    "\n",
    "<p> Supongamos que trabajamos por una empresa de ingeniería genética que quiere entender las evoluciones en las especies de semillas de grano. Tenemos a nuestra disposición el dataset 'semillas-trigo.csv'.</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right;width:45%;\">\n",
    "    <img src=\"images/trigo.jpeg\"></img>\n",
    "</div>\n",
    "<div style=\"clear:both; font-size:1px;\"></div>\n",
    "</div>\n",
    "\n",
    "<ul>\n",
    "<li>Cargar los datos en un DataFrame:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seeds_df = pd.read_csv('datasets/semillas-trigo.csv')\n",
    "\n",
    "# Suprimir la columna 'grain_variety' del dataset. Utilizaremos esta información solamente como referencia al final\n",
    "varieties = list(seeds_df.pop('grain_variety'))\n",
    "\n",
    "# Extraer los datos como un arreglo NumPy\n",
    "samples = seeds_df.values\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "seeds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>En SciPy, el método <i>linkage()</i> permite hacer clustering jerárquico o aglomerativo. Ver más detalles: <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\">linkage()</a> </p>\n",
    "\n",
    "<p> El clustering jerárquico consiste en calcular una distancia entre clusters. Los métodos más simples consisten en calcular una distancia entre 2 puntos referencias de cada clúster:  Nearest Point Algorithm (o 'single' en SciPy), Farthest Point Algorithm (or Voor Hees Algorithm o 'complete' en SciPy), UPGMA (o 'average' en Scipy), centroids. El <b>método Ward</b> se diferencia de las otras utilizando un algoritmo recursivo para encontrar un agrupamiento que minimiza la varianza en las distancias entre clústers.\n",
    "\n",
    "<ul>\n",
    "<li>Probar el método de clustering jerárquico con el método Ward y visualizar el resuldado con un Dendograma:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mergings = linkage(samples, method='...') #TODO\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "dendrogram(mergings,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6, \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Probar el método de clustering jerárquico con el método Ward y visualizar el resuldado con un Dendograma:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mergings2 = linkage(samples, method='complete')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "dendrogram(mergings2,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=..., #TODO\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Ejercicio 3: Distance-based clustering vs. Density-based clustering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li> En este ejercicio queremos explorar los datos del dataset3.csv y hemos elegido utilizar el algoritmo K-Means.\n",
    "<li> Cargar los datos:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cargar los datos:\n",
    "import pandas as pd\n",
    "dataframe3 = ... #TODO\n",
    "#Encontrar el mejor número de cluser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li> Encontrar cuál es el mejor número de clusters:\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Prueba por k entre 1 y 10\n",
    "num_k = range(1, 10)\n",
    "inertias = []\n",
    "\n",
    "for k in num_k:\n",
    "    model = KMeans(...) #TODO\n",
    "    model.fit(...) #TODO\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot ks vs inertias\n",
    "plt.plot(num_k, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(num_k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li> ¡El mejor número K parece ser 5! Clusterizemos con k=5 y visualizemos el resultado!\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "modelKmeans = KMeans(n_clusters=...) #TODO\n",
    "modelKmeans.fit(...) #TODO\n",
    "labels = modelKmeans.predict(dataframe3.values)\n",
    "\n",
    "plt.scatter(dataframe3.values[:,0], dataframe3.values[:,1], c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li> ¿Cuál es su opinión sobre el análisis?</li>\n",
    "<li> Probemos con el algoritmo DBSCAN:\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_colors(labels, colors='rgbykcm'):\n",
    "    colored_labels = []\n",
    "    for label in labels:\n",
    "        colored_labels.append(colors[label])\n",
    "    return colored_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "dataframe3 = pd.read_csv('datasets/dataset1.csv')\n",
    "\n",
    "# Fit a DBSCAN estimator\n",
    "estimator = DBSCAN(eps=0.4, min_samples=5)\n",
    "#dataframe3.values = df_circ[[\"x\", \"y\"]]\n",
    "estimator.fit(dataframe3)\n",
    "\n",
    "# Clusters are given in the labels_ attribute\n",
    "labels = estimator.labels_\n",
    "print(Counter(labels))\n",
    "\n",
    "colors = set_colors(labels)\n",
    "plt.scatter(dataframe3.values[:,0], dataframe3.values[:,1], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "dataframe3 = pd.read_csv('datasets/dataset3.csv')\n",
    "\n",
    "# Fit a DBSCAN estimator\n",
    "estimator = DBSCAN(eps=..., min_samples=...) #TODO\n",
    "#dataframe3.values = df_circ[[\"x\", \"y\"]]\n",
    "estimator.fit(dataframe3)\n",
    "\n",
    "# Clusters are given in the labels_ attribute\n",
    "labels = estimator.labels_\n",
    "print(Counter(labels))\n",
    "\n",
    "colors = set_colors(labels)\n",
    "plt.scatter(dataframe3.values[:,0], dataframe3.values[:,1], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #000000;\">\n",
    "<b>Preguntas:</b>\n",
    "<ol>\n",
    "<li> ¿De qué sirven los parametros epsilon y min_sample en DBSCAN?</li> \n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>4. Ejercicio 4: ¿Cómo agrupar datos no estructurados multi-dimensionales?</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>En el último ejercicio, vamos a explorar el agrupamiento de datos textuales con el algoritmo de Ward.\n",
    "En general, los algoritmos K-Means, Ward o DBSCAN son limitados para agrupar datos textuales, y es preferible utilizar otro protocolo no supervisado como Latent Dirichlet Allocation (LDA). Sin embargo este ejercicio nos servirá en particular para empezar a utilizar la librería NLTK y revisar algunos preprocesamientos sobre datos textuales.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Tenemos a nuestra disposición un dataset con 58 discursos políticos de los presidentes de Estados-Unidos. Cada uno corresponde al primer discurso que hace el presidente cuando entre en la Casa Blanca. Cargar el dataset 'speeches.csv':</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "#Cargar el dataset de speeches\n",
    "df_speeches = pd.read_csv('datasets/speeches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>SciKit-Learn viene con un API por defecto para transformar un dataset de textos brutos en una matrice donde cada texto es una representación vectorial del peso TFIDF de cada palabra.\n",
    "</ul>\n",
    "\n",
    "<img src=\"images/tfidf.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Transformar el dataset de textos en un matrice de pesos TFIDF:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df_speeches.values[:,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Calcular la distancia entre cada documento:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Agrupar los documentos con el algoritmo de Ward y la distancia entre documentos, y visualizar el resultado con un dendograma:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=df_speeches.values[:,1]);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "#plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Hacer lo mismo pero con un preprocesamiento de <i>Stemming</i> y <i>n-gram</i> antes:\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix2 = tfidf_vectorizer.fit_transform(df_speeches.values[:,4])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=df_speeches.values[:,1]);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "#plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
