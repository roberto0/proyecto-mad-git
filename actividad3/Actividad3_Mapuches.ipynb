{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Existe sesgo de medio para presentar las noticias respecto de los mapuches? (pluralismo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1493"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Cargar el dataset de tweets\n",
    "df_mapuches = pd.read_csv('datasets/sophia_mapuche_v2.csv',delimiter=\"|\", header=None)\n",
    "df_medios = pd.read_csv('datasets/sophia_medios-chile-2017.csv',delimiter=\"|\", header=None)\n",
    "\n",
    "#selección de los mensajes\n",
    "docs = df_mapuches[3].as_matrix()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mapuexpress         184\n",
      " biobio              125\n",
      " araucanianews        82\n",
      " eldesconcierto       69\n",
      " soytemuco            59\n",
      " AustralTemuco        58\n",
      " El_Ciudadano         49\n",
      " adnradiochile        41\n",
      " TVU_television       36\n",
      " laopinon             35\n",
      " rsumen               32\n",
      " CNNChile             27\n",
      " el_dinamo            26\n",
      " Temucodiario         26\n",
      " 24HorasTVN           24\n",
      " RedMiVoz             24\n",
      " GAMBA_CL             23\n",
      " latercera            22\n",
      " thecliniccl          22\n",
      " uchileradio          19\n",
      " elmostrador          18\n",
      " nacioncl             16\n",
      " mercuriovalpo        15\n",
      " T13                  13\n",
      " prensaopal           13\n",
      " FortinOficial        12\n",
      " CHVNoticiascl        12\n",
      " ELCLARINDECHILE      11\n",
      " rtierrabella         11\n",
      " corrupcionchile      10\n",
      "                    ... \n",
      " pinguinodiario        1\n",
      " carta_abierta         1\n",
      " sancarlosonline       1\n",
      " eltipografo           1\n",
      " elsoldeiquique        1\n",
      " elobservatodo         1\n",
      " vlnradio              1\n",
      " HDtuiter              1\n",
      " ElPeriodista          1\n",
      " eldefinido            1\n",
      " fisuramagazine        1\n",
      " elrancahuaso          1\n",
      " CCauqueninoCom        1\n",
      " PrensaChiloe          1\n",
      " ElPeriscopioCL        1\n",
      " ChillanOnline         1\n",
      " soyvalparaiso         1\n",
      " RADIOPALOMAFM         1\n",
      " bionoticiascl         1\n",
      " Publimetro_TV         1\n",
      " estrellachiloe        1\n",
      " lacuarta              1\n",
      " radiopudeto           1\n",
      " RadioGalactika        1\n",
      " radiosantamaria       1\n",
      " FMConquistador        1\n",
      " radiozero977          1\n",
      " MauleNoticias         1\n",
      " PatoFdez              1\n",
      " La_Segunda            1\n",
      "Name: 1, Length: 159, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_mapuches[1].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/roberto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/roberto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Extracting tf features for LDA...\n",
      "done in 0.405s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.150s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:812: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if doc_topic_distr != 'deprecated':\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n",
      "/Users/roberto/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110429208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "n_samples = 0.7\n",
    "n_features = 1000\n",
    "n_top_words = 20\n",
    "dataset = docs\n",
    "data_samples = dataset[:int(len(dataset)*n_samples)]\n",
    "test_samples = dataset[int(len(dataset)*n_samples):]\n",
    "\n",
    "\n",
    "stopWords = set(nltk.corpus.stopwords.words('spanish'))\n",
    "\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=7,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopWords)\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "t0 = time()\n",
    "tf_test = tf_vectorizer.transform(test_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "perplexity = []\n",
    "\n",
    "for i in range(1,12):\n",
    "    n_topics = i\n",
    "\n",
    "    #print(\"Fitting LDA models with tf features, \"\n",
    "    #      \"n_samples=%d, n_features=%d n_topics=%d \"\n",
    "    #      % (n_samples, n_features, n_topics))\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    t0 = time()\n",
    "    lda.fit(tf)\n",
    "\n",
    "    train_gamma = lda.transform(tf)\n",
    "    train_perplexity = lda.perplexity(tf, train_gamma)\n",
    "    \n",
    "    test_gamma = lda.transform(tf_test)\n",
    "    test_perplexity = lda.perplexity(tf_test, test_gamma)\n",
    "    \n",
    "    perplexity.append(test_perplexity)\n",
    "    \n",
    "    #print('sklearn preplexity: train=%.3f, test=%.3f' %\n",
    "    #      (train_perplexity, test_perplexity))\n",
    "\n",
    "    #print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    #\n",
    "plt.plot(range(1,12),perplexity,'-o')\n",
    "plt.xlabel('Numero de clusters')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.xticks(range(1,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aplicando Modelos Probabilistas de Tópicos y LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "stopWords = set(nltk.corpus.stopwords.words('spanish'))\n",
    "\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=7,\n",
    "                                stop_words=stopWords,tokenizer=tokenize_only, ngram_range=(1,1))\n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "diccionario= tf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "#Estimación de LDA con Bayes Variacional\n",
    "lda = LatentDirichletAllocation(n_components=4, max_iter=10,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "#Cálculo de índice de ajuste de los datos\n",
    "print(lda.perplexity(tf))\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "print_top_words(lda, diccionario, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopWords,tokenizer=tokenize_and_stem,ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(docs)\n",
    "num_k = range(1, 12)\n",
    "inertias = []\n",
    "\n",
    "for k in num_k:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k, n_init=10)\n",
    "\n",
    "    # Fit model to samples\n",
    "    model.fit(X)\n",
    "\n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "\n",
    "\n",
    "# Plot ks vs inertias\n",
    "plt.plot(num_k, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(num_k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando Kmeans y tf-idf para realizar un primer agrupamiento de los mensajes\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopWords,tokenizer=tokenize_and_stem,ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "from matplotlib.pylab import hist, show\n",
    "true_k = 4\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=10)\n",
    "model.fit(X)\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    cluster_words = np.array(terms)[order_centroids[i, :15]]\n",
    "    print('Cluster {}: {}'.format(i, ' '.join(cluster_words)))\n",
    "\n",
    "#Generando algunos gráficos de los resultados\n",
    "labels = model.predict(X)\n",
    "plt.scatter(range(1493),labels,c=labels)\n",
    "show()\n",
    "hist(labels,bins=np.arange(-0.5,5.6,1),alpha=0.5,rwidth=0.9)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es')\n",
    "\n",
    "def tokenize_lemmatize_filtering(text):\n",
    "    filtered_lemmas = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if (token.pos_ == 'NOUN' or token.pos_ == 'ADJ' or token.pos_ == \"VERB\"):\n",
    "            print(token.pos_)\n",
    "            filtered_lemmas.append(token.lemma_.lower())\n",
    "            \n",
    "        for w in doc.ents:\n",
    "            print(\"Named Entities:\",w,\"--->\",w.label)\n",
    "            \n",
    "        return filtered_lemmas\n",
    "    \n",
    "results = tokenize_lemmatize_filtering(\"El pequeño gato se encuantrea en la pieza.\")\n",
    "print(results)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
